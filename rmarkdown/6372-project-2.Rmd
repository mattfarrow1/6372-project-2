---
title: '6372: Project 2'
author: "Edward, Matt, Michael"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{r library-setup}
library(tidyverse)  # general data processing & plotting
library(here)       # relative location references
library(janitor)    # data cleanup tools
library(naniar)     # dealing with missing values
library(caret)      # misc functions for training and plotting classification and regression models
library(tidymodels)
library(GGally)     # for ggpairs
library(MASS)       # for LDA/QDA
```

# EDA From SAS

## Project Portions

-   EDA from SAS - **Edward**
-   Data Cleanup/Sampling - **Matt**
-   Model 1 - **Edward**
-   Model 2 Interaction - **???**
-   Model 3 LDA - **Matt**
-   Model 4 Tree - **Mike**
-   Model to Model (of Duration) - **???**

## Summary

### Variables that seem to matter

-   **Categorical**: Job, Education, Contact, Month, Marital, Day\_of\_week, Housing
-   **Continuous**: Duration, Campaign, Emp\_var\_rate, Euribor3m, Nr\_employed, Previous

### Variables that don't seem to matter

-   **Categorical**: Default, Loan
-   **Continuous**: Age, Cons\_price\_idx, Cons\_conf\_idx

### Variables that are junk/incomplete data that doesn't add explanatory or predictive capabilities

-   **Categorical**: Poutcome
-   **Continuous**: Pdays

### Missing values

-   For categorical, replace with the Mode for each one (listed below)

-   For continuous, only Pdays is missing, and we should delete it, but for completeness, replace with median (6)

-   VIF: Variable Inflation Factor

    -   Emp\_var\_rate, Euribor3m, Nr\_employed, Cons\_price\_idx, and Cons\_conf\_idx are all really just measures of time.
    -   They coordinate heavily with "what is the month"
    -   PCA should reveal that all of these continuous variables can be coordinated together and further, the month tells you the results as a category
    -   Alternately, ignore the month and use these variables to predict, but don't use both

## Categorical Variables

### PROC FREQ 

-   **Average**: No=88.7, %Yes=11.3%

-   **Total responses**: 41,188

-   **Job**

    -   Very strong

    -   Above average: student 31%, retired 25%, Unemployed 14%

    -   Below average: entrepreneur 8.5%, services 8%, blue-collar 7%

    -   Mode: Admin 10,422

    -   p-value \< 0.0001

-   **Marital**

    -   Above average: Single 14%

    -   Below average: married, 10.2%, divorced 10.3%

    -   Mode: married 24,928

    -   p-value \< 0.0001

-   **Education**

    -   Above average: illiterate 22%, university degree 14%

    -   Below average: Basic 9y 7.8%, Basic 6y 8.2%, Basic 4y 10.3%

    -   Mode: university degree 12,168

    -   p-value \< 0.0001

-   **Default** - Should definitely ignore this field

    -   Mode: No, 32,588 (only 3 yes' in the whole dataset)

    -   p-value \< 0.5054

-   **Housing**

    -   Above average: Yes 11.6%

    -   Below average: No 10.9%

    -   Mode: Yes 21,576

    -   p-value \< 0.02

-   **Loan** - Does not seem to be statistically significant

    -   Above average: No 11.3%

    -   Below average: Yes 10.9%

    -   Mode: No 33,950

    -   p-value = 0.3479

-   **Contact**

    -   Above average: Cellular 15%

    -   Below average: telephone 5%

    -   Mode: Cellular 26,144

    -   p-value \< 0.0001

-   **Month**

    -   Above average: Mar 51%, Dec 49%, Sep 45%, Oct 44%, Apr 20% [So basically, Sep-Mar except Nov is odd]

    -   Below average: May 6%, Jul 9%, Nov 10%, Jun 10.5%, Aug 10.6%

    -   Not in dataset: Jan, Feb

    -   Mode: May 13,769

    -   No Missing Data

    -   p-value \< 0.0001

-   **Day of Week**

    -   Above average: Thu 12.1%, Tue 11.8%, Wed 11.7%

    -   Below average: Mon 10%, Fri 10.8%

    -   Not in dataset: Sat, Sun

    -   Mode: Thu 8,623

    -   No Missing Data

    -   p-value \< 0.0001

-   **Poutcome**

    -   Somewhat misleading variable (when it's present which isn't very often, Yes is above average in both cases)

    -   Above average: Success 65%, Failure 14%

    -   Below average: NONE (see note above)

    -   Mode: Failure 4,252

    -   p-value \< 0.0001

## Continuous Variables

### PROC MEANS 

-   **Counts**

    -   No=36,548, Yes=4,640

    -   Only continuous variable with missing values is PDAYS

-   **Total responses**: 41,188

-   **age**

    -   No: Mean 39.9, Median 38

    -   Yes: Mean 40.9, Median 37

-   **duration**

    -   No: Mean 221, Median 164, Min 0

    -   Yes: Mean 553, Median 449, Min 37

    -   Longer the call, the more Yes increases

    -   If the call lasts 36 or fewer seconds, it is No 100% of the time

-   **campaign**

    -   No: Mean 2.6, Max 56

    -   Yes: Mean 2.0, Max 23

    -   Median: 2 (for both yes and no)

    -   Fewer the campaigns, the more Yes increases - If there are more than 23 campaigns, it is No 100% of the time

-   **pdays**

    -   Recommend we discard this variable

    -   Massive missing continuous variable

    -   No real difference between continuous values for yes and no, but:

    -   Misleading: when variable exists, Yes is the outcome for Subscribed 64% of the time

    -   Median: 6 (for both yes and no)

-   **previous**

    -   No: Mean 0.13

    -   Yes: Mean 0.49

    -   Median: 0 (for both yes and no)

-   **emp\_var\_rate**

    -   No: Mean 0.25, Median 1.1

    -   Yes: Mean -1.2, Median -1.8

    -   The lower the emp\_var\_rate, the more likely to get a Yes. In other words, negatively correlated.

-   **cons\_price\_idx**

    -   Doesn't seem to be very significant

    -   No: Mean 94, Median 94

    -   Yes: Mean 93, Median 93

-   **cons\_conf\_idx**

    -   Doesn't seem to be very significant

    -   No: Mean -41, Median -42

    -   Yes: Mean -40, Median -40

-   **euribor3m**

    -   No: Mean 3.8, Median 4.9

    -   Yes: Mean 2.1, Median 1.3

    -   The lower the euribor3m interest rate, the more likely to get a Yes. In other words, negatively correlated.

-   **nr\_employed**

    -   No: Mean 5176, Median 5196

    -   Yes: Mean 5095, Median 5099

    -   The lower the nr\_employed rate, the more likely to get a Yes. In other words, negatively correlated.

# Load Data

```{r load-data}
# Read data
bank <- read_delim(here("data - raw", "bank-additional", "bank-additional-full.csv"), 
                   ";", escape_double = FALSE, trim_ws = TRUE)

# Clean column names
bank <- clean_names(bank)

# Quick examination of the data
glimpse(bank)
head(bank)
```

## Attribute Information ([source](https://archive.ics.uci.edu/ml/datasets/Bank%20Marketing))

### Bank Client Data

1.  **age**
2.  **job**: type of job
3.  **marital**: marital status
4.  **education**
5.  **default**: has credit in default?
6.  **housing**: has housing loan?
7.  **loan**: has personal loan?

### Related with the Last Contact of the Current Campaign

8.  **contact**: contact communication type
9.  **month**: last contact month of year
10. **day\_of\_week**: last contact day of the week
11. **duration**: last contact duration, in seconds. Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

### Other Attributes

12. **campaign**: number of contacts performed during this campaign and for this client (includes last contact)
13. **pdays**: number of days that passed by after the client was last contacted from a previous campaign (999 means client was not previously contacted)
14. **previous**: number of contacts performed before this campaign and for this client
15. **poutcome**: outcome of the previous marketing campaign

### Social and Economic Context Attributes

16. **emp.var.rate**: employment variation rate - quarterly indicator
17. **cons.price.idx**: consumer price index - monthly indicator
18. **cons.conf.idx**: consumer confidence index - monthly indicator
19. **euribor3m**: euribor 3 month rate - daily indicator
20. **nr.employed**: number of employees - quarterly indicator

### Output Variable (Desired Target)

21. **y**: has the client subscribed to a term deposit?

# Address Missing Values

## Categorical Variables

```{r missing-categorical}
# Begin by replacing missing value placeholders with NA
bank_clean <- bank %>% 
  replace_with_na_all(condition = ~ .x == "unknown") %>% 
  replace_with_na(replace = list(pdays = 999)) %>% 
  replace_with_na(replace = list(poutcome = "nonexistent"))
  
# Replace each missing value in a categorical variable with the mode as
# determined during SAS EDA
bank_clean <- bank_clean %>% 
  mutate(job         = replace_na(job,         "admin."),
         marital     = replace_na(marital,     "married"),
         housing     = replace_na(housing,     "yes"),
         loan        = replace_na(housing,     "no"),
         contact     = replace_na(contact,     "cellular"),
         education   = replace_na(education,   "university.degree"),
         month       = replace_na(month,       "may"),
         day_of_week = replace_na(day_of_week, "thu"),
         poutcome    = replace_na(poutcome,    "failure"))

# Drop `default` column since there are only 3 "Yes" values
bank_clean <- bank_clean %>% 
  dplyr::select(-default)
```

## Continuous Variables

```{r missing-continuous}
# Only Pdays is missing, and we should delete it, but for completeness, replace
# with median (6)
bank_clean <- bank_clean %>% 
  mutate(pdays = replace_na(pdays, 6))
```

## Additional Cleanup

```{r}
# Rename response variable
bank_clean <- bank_clean %>% 
  rename(subscribed = y)

# Convert all character variables to factors
bank_clean <- bank_clean %>% 
  mutate(across(where(is_character),as_factor))
```

# Post-Cleanup Examination

```{r}
# Re-run reports
DataExplorer::create_report(bank_clean)
Hmisc::describe(bank_clean)
psych::describe(bank_clean)
skimr::skim(bank_clean)

# Reports from inspectdf (categorical variables and Pearson correlation
# coefficients)
bank_cat  <- inspectdf::inspect_cat(bank_clean)
bank_pear <- inspectdf::inspect_cor(bank_clean)
```

# Create Test/Train Splits and Save Data

```{r}
# Set seed
set.seed(123)

# Save full bank_clean data set
write_csv(bank_clean, here("data - output", "bank_clean.csv"))

# Create test/train data sets of full data
inTraining <- createDataPartition(bank_clean$subscribed, p = .75, list = FALSE)
training <- bank_clean[ inTraining,]
testing  <- bank_clean[-inTraining,]

# Save bank_clean test/train sets
write_csv(training, here("data - output", "bank_clean_train.csv"))
write_csv(testing, here("data - output", "bank_clean_test.csv"))

# Down-sample the data to get an equal number of yes and no responses
bank_clean_ds <- downSample(x = bank_clean[, 1:19],
                            y = bank_clean$subscribed)

# Double-check the number of yes/no responses
bank_clean_ds %>% 
  count(Class)

# Save downsampled data set
write_csv(bank_clean_ds, here("data - output", "bank_clean_downsampled.csv"))

# Create test/train data sets of downsampled data
inTraining <- createDataPartition(bank_clean_ds$Class, p = .75, list = FALSE)
training <- bank_clean_ds[ inTraining,]
testing  <- bank_clean_ds[-inTraining,]

# Save downsampled data test/train sets
write_csv(training, here("data - output", "bank_clean_ds_train.csv"))
write_csv(testing, here("data - output", "bank_clean_ds_test.csv"))
```


# Models

```{r}
library(caTools)
set.seed(123)
split <- sample.split(bank_clean$subscribed, SplitRatio = 0.60)
train_data <- subset(bank_clean, split == TRUE)
test_data <- subset(bank_clean, split == FALSE)

# Logistic regression model 1
model1 <-
  glm(
    subscribed ~ month + day_of_week + campaign + emp_var_rate + contact + poutcome + cons_conf_idx,
    data = train_data,
    family = binomial
  )

# Get summary of model1
summary(model1)

predict_test <- predict(model1, type = "response", newdata = test_data)
summary(predict_test)
table(test_data$subscribed, predict_test > 0.20) # return confusion matrix. repeat for various threshold values

library(ROCR)
# Use this code chunk to find ROC curve and calculate AUC
# ROCRpred <- prediction(predict_test, test_data$subscribed)
# as.numeric(performance(ROCRpred, "auc")@y.values)
# ROCRperf <- performance(ROCRpred, "tpr", "fpr")
# plot(ROCRperf, colorize = TRUE, print.cutoffs.at = seq(0, 1, 0.005), text.adj = c(-0.2, 1.7))

# Build and analyze regression tree
library(rpart)
library(rpart.plot)
cart1 <-
  rpart(
    subscribed ~ job + default + education + housing + loan + contact + poutcome,
    data = train_data,
    control = rpart.control(
      minsplit = 1,
      minbucket = 20,
      cp = 0.001
    )
  )

prediction_cart <- predict(cart1, newdata = test_data)
# table(test_data$subscribed, prediction_cart > 0.20) # return confusion matrix. repeat for various threshold values
summary(cart1)
prp(cart1)
```

# LDA/QDA Model

## Prep Data

```{r lda}
set.seed(123)

# Create LDA data set.
lda_data <- bank_clean %>% 
  dplyr::select(c("subscribed",
                  "cons_price_idx",
                  "cons_conf_idx",
                  "euribor3m"))

# Examine the data
skimr::skim(lda_data)

# Check number of yes/no responses
lda_data %>% 
  count(subscribed)

# Down-sample the data to get an equal number of yes and no responses
lda_ds <- downSample(x = lda_data[, 2:4],
                            y = lda_data$subscribed)

# Double-check the number of yes/no responses
lda_ds %>% 
  count(Class)

# Build test/train data sets
lda_indices <- sample(nrow(lda_ds), size = floor(0.75 * nrow(lda_ds)))
lda_train   <- as_tibble(lda_ds[lda_indices, ])
lda_test    <- as_tibble(lda_ds[-lda_indices, ])
```

## LDA using MASS

```{r}
# Run LDA
bank_lda <- lda(Class ~ cons_price_idx + cons_conf_idx + euribor3m, data = lda_train)

# Make predictions
bank_lda_predict <- predict(bank_lda, newdata = lda_test)$class

# Define truth
bank_lda_truth <- lda_test$Class

# Generate confusion matrix
confusionMatrix(bank_lda_predict,
                bank_lda_truth)
```

## LDA using Caret

```{r}
# Define control parameters for train
lda_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = T,
  summaryFunction = twoClassSummary,
  classProbs = T,
)

# LDA using Caret
bank_lda <-
  train(
    Class ~ cons_price_idx + cons_conf_idx + euribor3m,
    data = lda_train,
    method = "lda",
    trControl = lda_control
  )

# Make predictions
bank_lda_predict <- predict(bank_lda, newdata = lda_test)

# Define truth
bank_lda_truth <- lda_test$Class

# Generate confusion matrix
confusionMatrix(bank_lda_predict,
                bank_lda_truth)
```

## QDA

```{r qda}
# QDA
bank_qda <- qda(Class ~ cons_price_idx + cons_conf_idx + euribor3m, data = lda_train)

# Make predictions
bank_qda_predict <- predict(bank_qda, newdata = lda_test)$class

# Define truth
bank_qda_truth <- lda_test$Class

# Generate confusion matrix
confusionMatrix(bank_qda_predict,
                bank_qda_truth)
```

# Appendix

## R Code {.unnumbered}

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```
